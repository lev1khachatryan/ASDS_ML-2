{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">Common Loss Functions</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)\n",
    "\n",
    "<img src=\"Pics/main.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines learn by means of a loss function. It’s a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction. In this article we will go through several loss functions and their applications in the domain of machine/deep learning.\n",
    "\n",
    "***There’s no one-size-fits-all loss function to algorithms in machine learning***. There are various factors involved in choosing a loss function for specific problem such as type of machine learning algorithm chosen, ease of calculating the derivatives and to some degree the percentage of outliers in the data set.\n",
    "\n",
    "Broadly, loss functions can be classified into two major categories depending upon the type of learning task we are dealing with — Regression losses and Classification losses. In classification, we are trying to predict output from set of finite categorical values i.e Given large data set of images of hand written digits, categorizing them into one of 0–9 digits. Regression, on the other hand, deals with predicting a continuous value for example given floor area, number of rooms, size of rooms, predict the price of room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Regression Losses</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Mean Square Error/Quadratic Loss/L2 Loss</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d is: ['0.00000000', '0.16600000', '0.33300000']\n",
      "p is: ['0.00000000', '0.25400000', '0.99800000']\n",
      "rms error is: 0.3872849941150143\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_hat = np.array([0.000, 0.166, 0.333])\n",
    "y_true = np.array([0.000, 0.254, 0.998])\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    differences = predictions - targets\n",
    "    differences_squared = differences ** 2\n",
    "    mean_of_differences_squared = differences_squared.mean()\n",
    "    rmse_val = np.sqrt(mean_of_differences_squared)\n",
    "    return rmse_val\n",
    "\n",
    "print(\"d is: \" + str([\"%.8f\" % elem for elem in y_hat]))\n",
    "print(\"p is: \" + str([\"%.8f\" % elem for elem in y_true]))\n",
    "rmse_val = rmse(y_hat, y_true)\n",
    "print(\"rms error is: \" + str(rmse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Mean Absolute Error/L1 Loss</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. Like MSE, this as well measures the magnitude of error without considering their direction. Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d is: ['0.00000000', '0.16600000', '0.33300000']\n",
      "p is: ['0.00000000', '0.25400000', '0.99800000']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_hat = np.array([0.000, 0.166, 0.333])\n",
    "y_true = np.array([0.000, 0.254, 0.998])\n",
    "\n",
    "print(\"d is: \" + str([\"%.8f\" % elem for elem in y_hat]))\n",
    "print(\"p is: \" + str([\"%.8f\" % elem for elem in y_true]))\n",
    "\n",
    "def mae(predictions, targets):\n",
    "    differences = predictions - targets\n",
    "    absolute_differences = np.absolute(differences)\n",
    "    mean_absolute_differences = absolute_differences.mean()\n",
    "    return mean_absolute_differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Huber</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically used for regression. It’s less sensitive to outliers than the MSE as it treats error as square only inside an interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/6.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Huber(yHat, y, delta=1.):\n",
    "    return np.where(np.abs(y-yHat) < delta,.5*(y-yHat)**2 , delta*(np.abs(y-yHat)-0.5*delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Classification Losses</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Hinge Loss/Multi class SVM Loss</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple terms, the score of correct category should be greater than sum of scores of all incorrect categories by some safety margin (usually one). And hence hinge loss is used for maximum-margin classification, most notably for support vector machines. Although not differentiable, it’s a convex function which makes it easy to work with usual convex optimizers used in machine learning domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Cross Entropy Loss/Negative Log Likelihood/Log Loss</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most common setting for classification problems. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
    "\n",
    "In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when actual label is 1 (y(i) = 1), second half of function disappears whereas in case actual label is 0 (y(i) = 0) first half is dropped off. In short, we are just multiplying the log of the actual predicted probability for the ground truth class. An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss is: 0.7135329699138555\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = np.array([[0.25,0.25,0.25,0.25],\n",
    "                        [0.01,0.01,0.01,0.96]])\n",
    "targets = np.array([[0,0,0,1],\n",
    "                   [0,0,0,1]])\n",
    "def cross_entropy(predictions, targets, epsilon=1e-10):\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce_loss = -np.sum(np.sum(targets * np.log(predictions + 1e-5)))/N\n",
    "    return ce_loss\n",
    "\n",
    "cross_entropy_loss = cross_entropy(predictions, targets)\n",
    "print (\"Cross entropy loss is: \" + str(cross_entropy_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If M>2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/5.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Huber</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically used for regression. It’s less sensitive to outliers than the MSE as it treats error as square only inside an interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/6.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Huber(yHat, y, delta=1.):\n",
    "    return np.where(np.abs(y-yHat) < delta,.5*(y-yHat)**2 , delta*(np.abs(y-yHat)-0.5*delta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
