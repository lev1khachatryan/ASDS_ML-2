{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">The k-means Cluster Initialization Problem</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)\n",
    "\n",
    "<img src=\"pics/KMeaniniialization.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the greatest challenges in k-means clustering is positioning the initial cluster centers, or centroids, as close to optimal as possible, and doing so in an amount of time deemed reasonable. Traditional k-means utilizes a randomization process for initializing these centroids -- though this is not the only approach -- but poor initialization can lead to increased numbers of required clustering iterations to reach convergence, a greater overall runtime, and a less-efficient algorithm overall.\n",
    "\n",
    "If you consider that an iteration of the distance calculations required to compare each instance to each centroid takes constant time, then there are 2 aspects of the k-means algorithm that can be, and often are, targeted for optimization:\n",
    "\n",
    "* Centroid initialization, such that the initial cluster centers are placed as close as possible to the optimal cluster centers\n",
    "* Selection of the optimal value for k (the number of clusters, and centroids) for a particular dataset\n",
    "\n",
    "The actual clustering method of the k-means algorithm, which follows centroid initialization, and which is iterated upon until the “best” centroid location is found, is highly sensitive to the initial placement of centroids. The better the initial centroid placement, generally the fewer iterations are required, and the faster and more efficient the algorithm is. Research into improved centroid initialization has yielded both clever methods and improved results, but these methods are often resource intensive and/or time-consuming.\n",
    "\n",
    "What if the trade-off between computational efficiency and the effectiveness of centroid initialization methods was moot? What if a simple, deterministic approach which did not rely on randomization could be used for centroid initialization? And what if it was computationally simple enough that it relied on very few calculations, dramatically reducing the initialization time, lowering the number of clustering iterations required, and shortening the overall execution time of the k-means algorithm, all while producing comparable accuracy of results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">The Naive Sharding Centroid Initialization Algorithm</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "Classical k-means clustering utilizes random centroid initialization. In order for randomization to be properly performed, the entire space being occupied by all instances in all dimensions is first to be determined. This means that all instances in the dataset must be enumerated, and a record must be kept of the minimum and maximum values of each attribute, constituting the boundaries of the various planes in n-dimensional space, n being the number of attributes in the dataset. This can be a time-consuming process; the time required to perform randomization grows exponentially with increased dataset complexity, by way of both a larger number of instances and a larger number of attributes.  \n",
    "  \n",
    "The sharding centroid initialization algorithm proceeds in a very simple manner, and is primarily dependent on the calculation of a composite summation value reflecting all of the attribute values of an instance. Once this composite value is computed, it is used to sort the instances of the dataset. The dataset is then horizontally split into k pieces, or shards. Finally, the original attributes of each shard are independently summed, their mean is computed, and the resultant collection of rows of shard attribute mean values becomes the set of centroids to be used for initialization.\n",
    "\n",
    "Sharding initialization is expected to execute much quicker than random centroid initialization, especially considering that the time needed for randomly initializing centroids for increasingly complex datasets (more instances and more attributes) grows superlinearly, and can be extremely time-consuming given complex enough data; sharding initialization executes in linear time, and is not dependent on data complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/naive-sharding-1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of naive sharding is as follows:\n",
    "\n",
    "* Step 1: Sum the attribute (column) values for each instance (row) of a dataset, and prepend this new column of instance value sums to the dataset\n",
    "* Step 2: Sort the instances of the dataset by the newly created sum column, in ascending order\n",
    "* Step 3: Split the dataset horizontally into k equal-sized pieces, or shards\n",
    "* Step 4: For each shard, sum the attribute columns (excluding the column created in step 1), compute its mean, and place the values into a new row; this new row is effectively one of the centroids used for initialization\n",
    "* Step 5: Add each centroid row from each shard to a new set of centroid instances\n",
    "* Step 6: Return this set of centroid instances to the calling function for use in the k-means clustering algorithm\n",
    "\n",
    "\n",
    "As previously promised, the algorithm is very simple -- almost embarrassingly so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/naive-sharding-2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">Naive Sharding Implementation and Experimentation</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "First off, the above overview makes no mention of data preprocessing techniques, as they are of separate concern. However, as with clustering in general, outcomes will be affected by data values, and in the case of naive sharding, data scaling will be of particular importance, given that a composite value could be overly-influenced by a single overwhelming value's range otherwise.\n",
    "\n",
    "These concerns aside, below is a Python implementation of the naive sharding centroid initialization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, floor\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def naive_sharding(ds, k):\n",
    "    \"\"\"\n",
    "    Create cluster centroids using deterministic naive sharding algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : numpy array\n",
    "        The dataset to be used for centroid initialization.\n",
    "    k : int\n",
    "        The desired number of clusters for which centroids are required.\n",
    "    Returns\n",
    "    -------\n",
    "    centroids : numpy array\n",
    "        Collection of k centroids as a numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = np.shape(ds)[1]\n",
    "    m = np.shape(ds)[0]\n",
    "    centroids = np.mat(np.zeros((k,n)))\n",
    "\n",
    "    # Sum all elements of each row, add as col to original dataset, sort\n",
    "    composite = np.mat(np.sum(ds, axis=1))\n",
    "    ds = np.append(composite.T, ds, axis=1)\n",
    "    ds.sort(axis=0)\n",
    "\n",
    "    # Step value for dataset sharding\n",
    "    step = floor(m/k)\n",
    "\n",
    "    # Vectorize mean ufunc for numpy array\n",
    "    vfunc = np.vectorize(_get_mean)\n",
    "\n",
    "    # Divide matrix rows equally by k-1 (so that there are k matrix shards)\n",
    "    # Sum columns of shards, get means; these columnar means are centroids\n",
    "    for j in range(k):\n",
    "        if j == k-1:\n",
    "            centroids[j:] = vfunc(np.sum(ds[j*step:,1:], axis=0), step)\n",
    "        else:\n",
    "            centroids[j:] = vfunc(np.sum(ds[j*step:(j+1)*step,1:], axis=0), step)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def _get_mean(sums, step):\n",
    "    \"\"\"\n",
    "    Vectorizable ufunc for getting means of summed shard columns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sums : float\n",
    "        The summed shard columns.\n",
    "    step : int\n",
    "        The number of instances per shard.\n",
    "    Returns\n",
    "    -------\n",
    "    sums/step (means) : numpy array\n",
    "        The means of the shard columns.\n",
    "    \"\"\"\n",
    "\n",
    "    return sums/step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Scikit-learn's k-means algorithm implementation allows for the passing of initial centroids, we can demonstrate naive sharding without reinventing the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids using naive sharding (scaled data)\n",
      "--------------------------------------------\n",
      "[[0.17666667 0.25166667 0.07830508 0.06083333]\n",
      " [0.41944444 0.43       0.54949153 0.505     ]\n",
      " [0.69       0.64       0.77457627 0.80833333]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sklearn.preprocessing\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Separate data from target attributes\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Scale dataset\n",
    "minmax_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = minmax_scaler.fit_transform(X)\n",
    "\n",
    "# Use naive sharding to generate initial centroids\n",
    "centroids_scaled = naive_sharding(X_scaled, 3)\n",
    "print ('Centroids using naive sharding (scaled data)')\n",
    "print ('--------------------------------------------')\n",
    "print (centroids_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial centroids are plotted (in 2 dimensions) against the scaled Iris dataset below (centroids in yellow):\n",
    "<img src=\"pics/scaled-iris-naive-centroids.png\" />  \n",
    "  \n",
    "You can see that they are placed central to the action (at least in 2 dimensional space), relative to potential non-optimal random placement: any centroids placed in the empty upper right quadrant would certainly not be optimal, and any recovery through successive iterations of the algorithm would be compounded if multiple centroids ended up close together in such empty space.\n",
    "\n",
    "Of course, non-optimal centroid placement can be recovered from in k-means (one of its strengths), but what kind of recovery would be required, practically speaking? Let's perform the actual k-means clustering, first with random centroid initialization, and then with naive sharding centroid initialization, and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted\n",
      "---------\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1\n",
      " 1 1 2 1 1 1 1 1 2 1 2 1 2 1 1 2 2 1 1 1 1 1 2 2 1 1 1 2 1 1 1 2 1 1 1 2 1\n",
      " 1 2] \n",
      "\n",
      "Actual\n",
      "------\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2] \n",
      "\n",
      "Accuracy: 0.44666666666666666\n",
      "Iterations: 9\n",
      "Inertia: 6.982216473785234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "# Random initialization\n",
    "model_rand = KMeans(n_clusters=3, init='random')\n",
    "model_rand.fit(X_scaled)\n",
    "\n",
    "print ('Predicted')\n",
    "print ('---------')\n",
    "print (model_rand.labels_,'\\n')\n",
    "\n",
    "print ('Actual')\n",
    "print ('------')\n",
    "print (y,'\\n')\n",
    "\n",
    "print ('Accuracy:', sm.accuracy_score(y, model_rand.labels_))\n",
    "print ('Iterations:', model_rand.n_iter_)\n",
    "print ('Inertia:', model_rand.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted\n",
      "---------\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 2 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 1 2\n",
      " 2 1] \n",
      "\n",
      "Actual\n",
      "------\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2] \n",
      "\n",
      "Accuracy: 0.8866666666666667\n",
      "Iterations: 3\n",
      "Inertia: 6.982216473785234\n"
     ]
    }
   ],
   "source": [
    "# Naive sharding initialization\n",
    "model_ns = KMeans(n_clusters=3, init=centroids_scaled)\n",
    "model_ns.fit(X_scaled)\n",
    "\n",
    "print ('Predicted')\n",
    "print ('---------')\n",
    "print (model_ns.labels_,'\\n')\n",
    "\n",
    "print ('Actual')\n",
    "print ('------')\n",
    "print (y,'\\n')\n",
    "\n",
    "print ('Accuracy:', sm.accuracy_score(y, model_ns.labels_))\n",
    "print ('Iterations:', model_ns.n_iter_)\n",
    "print ('Inertia:', model_ns.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bearing in mind that the ground truth of this dataset is known, and that we have proceeded accordingly, both centroid initialization methods result in an accuracy of approximately 89%, and share the same inertia, or within-cluster sum-of-squares. The only difference is that naive sharding took 1 iteration of k-means to get to the result, while random initialization took 9.\n",
    "<img src=\"pics/iris-ds-naive-optimal-centroids.png\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who cares, though, right? Well, on a toy dataset like Iris -- with its 150 instances -- that iteration difference and the processing time that comes with it, may not be important, but with a much larger dataset the savings in time -- with equivalent of competitive accuracy of results -- could be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's investigate a slightly larger dataset, 3D Road Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids using naive sharding (scaled data)\n",
      "--------------------------------------------\n",
      "[[0.13546377 0.11747167 0.05458445 0.07378088]\n",
      " [0.25125091 0.25433351 0.14654911 0.09003291]\n",
      " [0.50881844 0.39242227 0.22776759 0.10992592]\n",
      " [0.59676316 0.48547428 0.31513051 0.13666101]\n",
      " [0.62017295 0.55034077 0.37331935 0.16647871]\n",
      " [0.65406449 0.58429265 0.42799943 0.20076948]\n",
      " [0.70674852 0.6121824  0.52515724 0.23930599]\n",
      " [0.78915972 0.66302398 0.62559139 0.28339069]\n",
      " [0.87849087 0.72028317 0.73596579 0.35175889]\n",
      " [0.95252989 0.81382297 0.85959953 0.50055848]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "# Load the 3d road network dataset\n",
    "X = genfromtxt('data/3D_spatial_network.txt', delimiter=',')\n",
    "\n",
    "# Scale dataset\n",
    "minmax_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = minmax_scaler.fit_transform(X)\n",
    "\n",
    "# Use naive sharding to generate initial centroids\n",
    "centroids_scaled = naive_sharding(X_scaled,10)\n",
    "print ('Centroids using naive sharding (scaled data)')\n",
    "print ('--------------------------------------------')\n",
    "print (centroids_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above experiments on this dataset provides the following insight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random initialization\n",
      "---------------------\n",
      "Iterations: 24\n",
      "Inertia: 17548.067285693127\n",
      "Elapsed time: 55.340932900000006 \n",
      "\n",
      "Naive sharding initialization\n",
      "-----------------------------\n",
      "Iterations: 12\n",
      "Inertia: 17584.08932200825\n",
      "Elapsed time: 2.709294200000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sm\n",
    "import time\n",
    "\n",
    "# Time entire random init k-means algorithm execution process\n",
    "t0_rand = time.clock()\n",
    "\n",
    "# Random initialization\n",
    "model_rand = KMeans(n_clusters=10, init='random')\n",
    "model_rand.fit(X_scaled)\n",
    "\n",
    "t1_rand = time.clock()\n",
    "elapsed_rand = t1_rand - t0_rand\n",
    "\n",
    "# Time entire naive sharding k-means algorithm execution process\n",
    "t0_ns = time.clock()\n",
    "\n",
    "# Naive sharding initialization\n",
    "centroids_scaled = naive_sharding(X_scaled,10) # recalculate for fairness of measure\n",
    "model_ns = KMeans(n_clusters=10, init=centroids_scaled)\n",
    "model_ns.fit(X_scaled)\n",
    "\n",
    "t1_ns = time.clock()\n",
    "elapsed_ns = t1_ns - t0_ns\n",
    "\n",
    "print ('Random initialization')\n",
    "print ('---------------------')\n",
    "print ('Iterations:', model_rand.n_iter_)\n",
    "print ('Inertia:', model_rand.inertia_)\n",
    "print ('Elapsed time:', elapsed_rand, '\\n')\n",
    "\n",
    "print ('Naive sharding initialization')\n",
    "print ('-----------------------------')\n",
    "print ('Iterations:', model_ns.n_iter_)\n",
    "print ('Inertia:', model_ns.inertia_)\n",
    "print ('Elapsed time:', elapsed_ns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the ground truth of this dataset is unknown, a nominal value of 10 was used for k.\n",
    "\n",
    "A few items of note:\n",
    "\n",
    "* The inertia of the model built from naive sharding centroids was minimally lower than that of the randomly initialized centroid model\n",
    "* Naive sharding resulted in less than half of the required clustering iterations to reach optimal centroid placement (11 vs. 29)\n",
    "* Perhaps most importantly, the elapsed time of the random centroid model was approximately 28 times that of the naive centroid model, suggesting that the real time savings of the proposed algorithm is in the placement of the initial centroids, as opposed to the clustering iterations -- though both may be a factor with datasets of significant enough size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\">Conclusion</div>\n",
    "---------------------------------------------------------------------\n",
    " \n",
    "There are no exuberant claims being made that this method of centroid initialization should supplant random initialization. Only a few simple experiments have been presented here, but significantly more testing during this research resulted in further support of the naive sharding centroid initialization method. There were definitely dataset/k value combinations during research which resulted in fewer required iterations from random centroid initialization; however, in the majority of such cases the number of iterations was competitive, and often the overall execution time was still better for naive sharding initialization. Surprisingly, in most of these cases the inertia was actually lower for naive sharding as well, suggesting that the deterministic approach may be providing empirically better centroids from a distance perspective.\n",
    "\n",
    "Centroid initialization, much like real estate, is mostly about location, location, location. The naive centroid initialization method provides an alternative approach to this critical clustering step which may help identify the best starting location for centroids, and do so relatively quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
