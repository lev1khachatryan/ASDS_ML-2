{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div align=\"center\">Simple Linear Regression</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  It is a very straightforward approach for predicting a **quantitative** response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y . Mathematically, we can write this linear relationship as  \n",
    "  \n",
    "  $Y \\approx \\beta_{0} + \\beta_{1}X \\tag 1$  \n",
    "In Equation (1), $\\beta_{0}$ and $\\beta_{1}$ are two unknown constants that represent the **intercept** and **slope** terms in the linear model. Together, $\\beta_{0}$ and $\\beta_{1}$ are known as the model coefficients or parameters. Once we have used our training data to produce estimates $\\hat{\\beta_{0}}$ and $\\hat{\\beta_{1}}$ for the model coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Estimating the Coefficients</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, $\\beta_{0}$ and $\\beta_{1}$ are unknown. So before we can use (1) to make predictions, we must use data to estimate the coefficients. Let  \n",
    "  \n",
    "$(x_{1}, y_{1}),  (x_{2}, y_{2}) , . . . , (x_{n}, y_{n})\\tag*{}$\n",
    "  \n",
    "represent n observation pairs, each of which consists of a measurement of X and a measurement of Y. Our goal is to obtain coefficient estimates $\\hat{\\beta_{0}}$ and $\\hat{\\beta_{1}}$ such that the linear model (1) fits the available data well—that is, so that $y_{i} \\approx \\hat{\\beta_{0}} + \\hat{\\beta_{0}}x_{i}$ for i = 1, . . . , n. In other words, we want to find an intercept $\\hat{\\beta_{0}}$ and a slope $\\hat{\\beta_{1}}$ such that the resulting line is as close as possible to the n data points. There are a number of ways of measuring closeness. However, by far the most common approach involves minimizing the least squares criterion, and we take that approach in this chapter. Alternative approaches will be considered in next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/1.png\" />\n",
    "FIGURE 1. For the Advertising data, the least squares fit for the regression of sales onto TV is shown. The fit is found by minimizing the sum of squared errors. Each grey line segment represents an error, and the fit makes a compromise by averaging their squares. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{y_{i}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}x_{i}$ be the prediction for Y based on the ith value of X. Then $e_{i} = y_{i} - \\hat{y_{i}}$ represents the ith residual—this is the difference between the ith observed response value and the ith response value that is predicted by our linear model. We define the residual sum of squares (RSS) as  \n",
    "  \n",
    "  $RSS = e_{1}^2 + e_{2}^2 + e_{n}^2,\\tag 2$\n",
    "  \n",
    "The least squares approach chooses $\\hat{\\beta_{0}}$ and $\\hat{\\beta_{1}}$ to minimize the $RSS$. Using some calculus, one can show that the minimizers are \n",
    "  \n",
    "  $\\hat{\\beta_{1}} = \\frac {\\sum_{i = 1}^{n} (x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sum_{i = 1}^{n} (x_{i}-\\overline{x})^2}  \\tag 3$\n",
    "  \n",
    "  $\\beta_{0} = \\overline{y} - \\hat{\\beta_{1}} \\overline{x}  \\tag*{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proove:\n",
    "<img src=\"pics/2.png\" style=\"width: 170px;\"/>\n",
    "<img src=\"pics/3.png\" style=\"width: 300px;\"/>\n",
    "<img src=\"pics/4.png\" style=\"width: 300px;\"/>\n",
    "<img src=\"pics/5.png\" style=\"width: 300px;\"/>\n",
    "<img src=\"pics/6.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Assessing the Accuracy of the Coefficient Estimates</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the true relationship between X and Y takes the form $Y = f(X) + \\epsilon$ for some unknown function f, where $\\epsilon$ is a mean-zero random error term. If f is to be approximated by a linear function, then we can write this relationship as  \n",
    "  \n",
    "  $Y = \\beta_{0} + \\beta_{1}X + \\epsilon \\tag*{}$\n",
    "  \n",
    "Here $\\beta_{0}$ is the intercept term—that is, the expected value of Y when X = 0, and $\\beta_{1}$ is the slope—the average increase in Y associated with a one-unit increase in X. The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in Y , and there may be measurement error. We typically assume that the error term is independent of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wonder how close $\\hat{\\beta_{0}}$ and $\\hat{\\beta_{1}}$ are to the true values $\\beta_{0}$ and $\\beta_{1}$. To compute the standard errors associated with $\\hat{\\beta_{0}}$ and $\\hat{\\beta_{1}}$, we use the following formulas:\n",
    "<img src=\"pics/7.png\" style=\"width: 600px;\"/>\n",
    "where $\\sigma^2 = Var(\\epsilon)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these formulas to be strictly valid, we need to assume that the errors $\\epsilon_{i}$ for each observation are uncorrelated with common variance $\\sigma^2$. This is clearly not true in Figure (1), but the formula still turns out to be a good approximation. Notice in the formula that $SE(\\hat{\\beta_{1}})$ is smaller when the $x_{i}$ are more spread out; intuitively we have more leverage to estimate a slope when this is the case. We also see that $SE(\\hat{\\beta_{0}})$ would be the same as $SE(\\hat{\\mu})$ if $\\overline{x}$ were zero (in which case $\\hat{\\beta_{0}}$ would be equal to $\\overline{y}$). In general, $\\sigma^2$ is not known, but can be estimated from the data. The estimate of $\\sigma$ is known as the **residual standard error**, and is given by the formula:\n",
    "  \n",
    "  $RSE = \\sqrt{RSE/(n-2)}   \\tag*{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. For linear regression, the 95% confidence interval for $\\beta_{1}$ approximately takes the form\n",
    "  \n",
    "  $\\hat{\\beta_{1}} \\pm 2* SE(\\hat{\\beta_{1}}) \\tag*{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of  \n",
    "  \n",
    "  $H_{0} : There\\;is\\;no\\;relationship\\;between\\;X\\;and\\;Y \\tag*{}$\n",
    "  $H_{a} : There\\;is\\;some\\;relationship\\;between\\;X\\;and\\;Y \\tag*{}$\n",
    "  \n",
    "In practice, we compute a t-statistic given by\n",
    "  \n",
    "  $t=\\frac{\\hat{\\beta_{1}} - 0}{SE(\\hat{\\beta_{1}})} \\tag*{}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the other sources:\n",
    "  \n",
    "**How to calculate p-value for multivariate linear regression**\n",
    "<img src=\"pics/12.png\"/>\n",
    "<img src=\"pics/8.png\"/>\n",
    "<img src=\"pics/9.png\"/>\n",
    "<img src=\"pics/10.png\"/>\n",
    "<img src=\"pics/11.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Assessing the Accuracy of the Model</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have rejected the null hypothesis in favor of the alternative hypothesis, it is natural to want to quantify the extent to which the model fits the data. The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error $RSE$ and the $R^2$ statistic.\n",
    "<img src=\"pics/13.png\" style=\"width: 400px;\"/>\n",
    "Roughly speaking, it is the average amount that the response will deviate from the true regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSE provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of Y , it is not always clear what constitutes a good RSE. The $R^2$ statistic provides an alternative measure of fit. It takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1, and is independent of the scale of Y .  \n",
    "<img src=\"pics/14.png\" style=\"width: 300px;\"/>\n",
    "where $TSS=\\sum_{i=1}^{n}(y_{i}-\\overline{y})^2 $ is the total sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/15.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------\n",
    "# <div align=\"center\">Multiple Linear Regression</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, suppose that we have p distinct predictors. Then the multiple linear regression model takes the form:  \n",
    "  \n",
    "<img src=\"pics/16.png\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Estimating the Regression Coefficients</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/17.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/18.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Other Sources**\n",
    "<img src=\"pics/20.png\"/>\n",
    "<img src=\"pics/21.png\"/>\n",
    "<img src=\"pics/31.png\"/>\n",
    "<img src=\"pics/23.png\"/>\n",
    "<img src=\"pics/24.png\"/>\n",
    "<img src=\"pics/25.png\"/>\n",
    "<img src=\"pics/26.png\"/>\n",
    "<img src=\"pics/27.png\"/>\n",
    "<img src=\"pics/28.png\"/>\n",
    "<img src=\"pics/29.png\"/>\n",
    "<img src=\"pics/30.png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
